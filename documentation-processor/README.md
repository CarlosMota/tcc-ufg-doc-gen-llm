# Documentation Processor

Processador de documenta√ß√£o para an√°lise e inser√ß√£o de dados em banco vetorial.

## üìã Pr√©-requisitos

- Micromamba ou Conda instalado
- Python 3.11+

## üöÄ Instala√ß√£o do Ambiente

### 1. Criar o ambiente conda/micromamba

```bash
micromamba env create -f environment.yml
```

### 2. Ativar o ambiente

```bash
micromamba activate documentation-processor
```

### 3. Instalar pacotes Python adicionais via pip

‚ö†Ô∏è **Importante**: Instalar os pacotes na ordem abaixo para evitar conflitos de depend√™ncias:

```bash
# 1. Instalar langchain primeiro (framework base)
pip install langchain==0.1.0

# 2. Instalar integra√ß√£o OpenAI
pip install langchain-openai==0.0.2

# 3. Instalar ChromaDB (banco de dados vetorial)
pip install chromadb==0.4.22

# 4. Instalar Sentence Transformers (√∫ltimo para evitar conflitos)
pip install sentence-transformers==2.2.2

# 5. Fix de compatibilidade do huggingface
pip install "huggingface-hub<0.20.0"
```

### 4. Verificar instala√ß√£o

Execute o script de verifica√ß√£o:

```bash
python -c "
import sys
packages = ['langchain', 'langchain_openai', 'chromadb', 'sentence_transformers']
for package in packages:
    try:
        __import__(package)
        print(f'‚úÖ {package}: Instalado com sucesso')
    except ImportError as e:
        print(f'‚ùå {package}: Erro - {e}')
"
```

## üì¶ Pacotes Instalados

### Depend√™ncias Conda (ambiente base):

- **python=3.11**: Linguagem Python
- **numpy**: Computa√ß√£o num√©rica
- **pandas**: Manipula√ß√£o de dados
- **pydantic**: Valida√ß√£o de dados
- **pyyaml**: Parser YAML
- **python-dotenv**: Gerenciamento de vari√°veis de ambiente
- **requests**: Requisi√ß√µes HTTP
- **httpx**: Cliente HTTP moderno
- **aiofiles**: Suporte a arquivos ass√≠ncronos
- **tiktoken**: Tokeniza√ß√£o para modelos OpenAI

### Depend√™ncias Pip (instala√ß√£o manual):

- **langchain**: Framework para aplica√ß√µes LLM
- **langchain-openai**: Integra√ß√£o com OpenAI
- **chromadb**: Banco de dados vetorial
- **sentence-transformers**: Modelos de embeddings

## üîß Solu√ß√£o de Problemas

### Erro: "Could not build wheels"

```bash
pip install --upgrade pip setuptools wheel
pip install package-name --no-cache-dir
```

### Erro: Conflitos de depend√™ncias

```bash
# Desinstalar tudo e reinstalar na ordem correta
pip uninstall langchain langchain-openai chromadb sentence-transformers -y
pip install langchain==0.1.0
pip install langchain-openai==0.0.2
pip install chromadb==0.4.22
pip install sentence-transformers==2.2.2
pip install "huggingface-hub<0.20.0"
```

## üîÑ Pipeline de normaliza√ß√£o e inser√ß√£o no ChromaDB

O script `documentation_processor` executa tr√™s etapas: l√™ os JSONs gerados pelos parsers em `data/01-parser/*`, normaliza os dados adicionando um `id`, grava em `data/02-normalization/` e insere o texto/metadata no ChromaDB.

### Executar

```bash
cd documentation-processor
python -m documentation_processor --collection documentation
```

### Estrutura esperada

- Entrada: `data/01-parser/<nome_parser>/*.json`
- Normaliza√ß√£o: `data/02-normalization/<nome_parser>/*__<id>.json`
- Chroma persistente: `data/03-vector-store/`

### Vari√°veis de ambiente √∫teis

- `PARSER_INPUT_DIR`: caminho alternativo para as pastas de parser.
- `NORMALIZATION_DIR`: destino dos arquivos normalizados.
- `CHROMA_DB_DIR`: pasta de persist√™ncia do ChromaDB.
- `CHROMA_COLLECTION`: nome padr√£o da cole√ß√£o (override do par√¢metro `--collection`).
- `EMBEDDING_MODEL_NAME`: modelo `sentence-transformers` usado para embeddings (padr√£o: `all-MiniLM-L6-v2`).

### Sa√≠da salva em JSON

Cada arquivo normalizado inclui:

- `id` est√°vel (UUID5) combinando parser + contexto + assinatura.
- Campos originais relevantes (`namespace`, `class_name`, `method_name`, `signature`, `content`, docs etc).
- `embedding_text` j√° concatenado (assinatura, coment√°rios XML, c√≥digo e constantes).
- `metadata` extra preservando chaves desconhecidas do parser.

### Erro: Vers√µes incompat√≠veis

Use as vers√µes especificadas acima ou consulte a documenta√ß√£o oficial dos pacotes.
